# Micro tutorial on how to run and scale hyperparameter optimization with LightGBM and Tune

![tune_overview](https://docs.ray.io/en/latest/_images/tune_overview.png)

## It is for you if:
* you model structured data using LightGBM (classification or regression tasks),
* want to run hyper-parameter optimization and scale it up quickly.

## What will you do?
* Run HPO job with LightGBM on the structured data.
* Configure _search algorithm_ and _scheduler_ for more afficient HPO job.
* Configure Tune to better utilize available compute resources.

## What will you learn?
* Few bits about Ray and Tune fundamentals.
* How to use Tune to run HPO jobs.
* Few more bits about _search algorithm_ and _scheduler_, that help define how the HPO job should be executed.

# Where to start?
Go ahead and open [ray_tune_micro_tutorial.ipynb](ray_tune_micro_tutorial.ipynb)

But before that make sure that you have an environment ready. Please follow the instructions on [Setup an environment](environment_setup.md) page (5 minutes read).

## What to do next?
* Have a closer look at [Tune docs](https://docs.ray.io/en/latest/tune/index.html) to learn more about other [search algorithms]() and [schedulers]().

## How to connect with community, learn more, join other trainings?
* Feel free to reach out on [Ray-distributed Slack](https://ray-distributed.slack.com/archives/C011ML23W5B). Join `#tutorials` channel, say hello and ask questions.
