{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "source": [
    "# Micro tutorial on how to run and scale hyperparameter optimization with LightGBM and Tune\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune_overview.png\" alt=\"Tune and integrations\" width=\"500\">\n",
    "\n",
    "Aug 2022. San Francisco, CA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: single LightGBM training session\n",
    "<img src=\"https://lightgbm.readthedocs.io/en/latest/_images/LightGBM_logo_black_text.svg\" alt=\"LightGBM Logo\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LightGBM](https://lightgbm.readthedocs.io) is a gradient boosting framework that uses tree based learning algorithms. It has Python API for model training and evaluation. Trained model can be inspected in multiple ways including visualizations like feature importance or trees plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=7707)\n",
    "\n",
    "train_data = lgb.Dataset(data=X_train, label=y_train, free_raw_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use [digits dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) (classification) and create LightGBM Dataset object that will be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training parameters for single training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"num_class\": 10,\n",
    "    \"num_leaves\": 5,          # max number of leaves in one tree\n",
    "    \"learning_rate\": 0.001,   # boosting learning rate\n",
    "    \"feature_fraction\": 0.5,  # fraction of features on each iteration\n",
    "    \"bagging_fraction\": 0.5,  # like \"feature_fraction\", but this will randomly select part of data without resampling\n",
    "    \"bagging_freq\": 50,       # frequency for bagging\n",
    "    \"max_depth\": 2,           # max depth of the tree\n",
    "    \"verbose\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and train LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize booster\n",
    "gbm = lgb.Booster(params=training_parameters, train_set=train_data)\n",
    "\n",
    "# Train booster for 200 iterations\n",
    "for i in range(200):\n",
    "    gbm = lgb.train(\n",
    "        params=training_parameters,\n",
    "        train_set=train_data,\n",
    "        num_boost_round=1,\n",
    "        init_model=gbm,\n",
    "        keep_training_booster=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report accuracy on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on valid set: 0.7694, after 200 iterations.\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(gbm.predict(X_valid), axis=1)\n",
    "acc = accuracy_score(y_true=y_valid, y_pred=y_pred)\n",
    "print(f\"Accuracy on valid set: {acc:.4f}, after {gbm.current_iteration()} iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "We just ran single LightGBM training session. To do that we prepared dataset and training hyperparameters.\n",
    "\n",
    "#### Next\n",
    "Let's have a closer look at Tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2: Tune quickstart\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune.png\" alt=\"Tune logo\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Tune\n",
    "#### Key concepts\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune_flow.png\" alt=\"Tune key concepts\" width=\"800\">\n",
    "\n",
    "Learn more about it from the [Key concepts](https://docs.ray.io/en/latest/tune/key-concepts.html) docs page.\n",
    "\n",
    "#### Scaling of the tuning jobs\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/0*EZKV8RTgDt0NfL49\" alt=\"scaling\" width=\"600\">\n",
    "\n",
    "Learn more from the Richard Liaw et al. [paper](https://arxiv.org/abs/1807.05118) introducing Tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 12:29:01,753\tINFO services.py:1470 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-08-08_12-28-59_999398_3126/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-08-08_12-28-59_999398_3126/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2022-08-08_12-28-59_999398_3126',\n",
       " 'metrics_export_port': 61988,\n",
       " 'gcs_address': '127.0.0.1:52563',\n",
       " 'address': '127.0.0.1:52563',\n",
       " 'node_id': '3c651efebd6f7425d1b5019fbb27b67fe6aaec433c28d62b62ebc459'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "cluster_info = ray.init(num_cpus=8)\n",
    "cluster_info.address_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `ray.init()` starts Ray runtime on a single machine. By default it will utilize all cores available on the machine. Here, we parametrized it to use `num_cpus=8`.\n",
    "* Check [configuring ray](https://docs.ray.io/en/latest/ray-core/configure.html#configuring-ray) page for more in depth analysis of available options.\n",
    "* This runtime will be used for all tuning jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"num_class\": 10,\n",
    "    \"num_leaves\": tune.choice([2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 100]),\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"feature_fraction\": tune.uniform(0.5, 0.999),\n",
    "    \"bagging_fraction\": 0.5,\n",
    "    \"bagging_freq\": tune.randint(1, 50),\n",
    "    \"max_depth\": tune.randint(1, 11),\n",
    "    \"verbose\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice that you can freely mix tune functions for defining search space (i.e. `tune.randint(1, 11)`) with fixed values (i.e. `\"num_class\": 10`).\n",
    "* [Search space API](https://docs.ray.io/en/latest/tune/api_docs/search_space.html) has variety of functions that you can use to define your search space in a way that suits your needs. Function used above are just few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(training_params, checkpoint_dir=None):\n",
    "    train_data = lgb.Dataset(data=X_train, label=y_train, free_raw_data=False)\n",
    "\n",
    "    # Initialize booster\n",
    "    gbm = lgb.Booster(params=training_params, train_set=train_data)\n",
    "\n",
    "    # Train booster for 200 iterations\n",
    "    for i in range(200):\n",
    "        gbm = lgb.train(\n",
    "            params=training_params,\n",
    "            train_set=train_data,\n",
    "            num_boost_round=1,\n",
    "            init_model=gbm,\n",
    "            keep_training_booster=True,\n",
    "        )\n",
    "\n",
    "        y_pred = np.argmax(gbm.predict(X_valid), axis=1)\n",
    "        acc = accuracy_score(y_true=y_valid, y_pred=y_pred)\n",
    "\n",
    "        # Send accuracy back to Tune\n",
    "        tune.report(valid_acc=acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Trainable (`train_lgbm`) is a function that will be evaluated multiples times during tuning.\n",
    "* LightGBM model training logic is the same as in the \"vanilla\" example above.\n",
    "* It is executed on a separate Ray Actor (process), so we need to communicate the performance of the model back to Tune (which is on the main Python process). Here, `tune.report()` comes into play - it sends the performance value back to Tune. In this case it is `acc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run hyperparameter tuning, single trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-08 12:29:04 (running for 00:00:01.19)<br>Memory usage on this node: 12.1/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/17.23 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/kamil/ray_results/train_lgbm_2022-08-08_12-29-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  bagging_freq</th><th style=\"text-align: right;\">  feature_fraction</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  num_leaves</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_lgbm_5c1d5_00000</td><td>RUNNING </td><td>127.0.0.1:3160</td><td style=\"text-align: right;\">             7</td><td style=\"text-align: right;\">          0.887462</td><td style=\"text-align: right;\">     0.00176246</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">          20</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_lgbm_5c1d5_00000:\n",
      "  date: 2022-08-08_12-29-04\n",
      "  done: false\n",
      "  experiment_id: d8826c2afcf148dc93bc8a79a0075820\n",
      "  hostname: MacBook.local\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 3160\n",
      "  time_since_restore: 0.010824918746948242\n",
      "  time_this_iter_s: 0.010824918746948242\n",
      "  time_total_s: 0.010824918746948242\n",
      "  timestamp: 1659986944\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 5c1d5_00000\n",
      "  valid_acc: 0.825\n",
      "  warmup_time: 0.0021009445190429688\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-08 12:29:08 (running for 00:00:05.21)<br>Memory usage on this node: 12.0/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/17.23 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/kamil/ray_results/train_lgbm_2022-08-08_12-29-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  bagging_freq</th><th style=\"text-align: right;\">  feature_fraction</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  num_leaves</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  valid_acc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_lgbm_5c1d5_00000</td><td>RUNNING </td><td>127.0.0.1:3160</td><td style=\"text-align: right;\">             7</td><td style=\"text-align: right;\">          0.887462</td><td style=\"text-align: right;\">     0.00176246</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         4.01442</td><td style=\"text-align: right;\">   0.886111</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_lgbm_5c1d5_00000:\n",
      "  date: 2022-08-08_12-29-09\n",
      "  done: false\n",
      "  experiment_id: d8826c2afcf148dc93bc8a79a0075820\n",
      "  hostname: MacBook.local\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 3160\n",
      "  time_since_restore: 5.012823820114136\n",
      "  time_this_iter_s: 0.08013081550598145\n",
      "  time_total_s: 5.012823820114136\n",
      "  timestamp: 1659986949\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 112\n",
      "  trial_id: 5c1d5_00000\n",
      "  valid_acc: 0.8888888888888888\n",
      "  warmup_time: 0.0021009445190429688\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-08 12:29:13 (running for 00:00:10.23)<br>Memory usage on this node: 12.0/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/17.23 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/kamil/ray_results/train_lgbm_2022-08-08_12-29-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  bagging_freq</th><th style=\"text-align: right;\">  feature_fraction</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  num_leaves</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  valid_acc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_lgbm_5c1d5_00000</td><td>RUNNING </td><td>127.0.0.1:3160</td><td style=\"text-align: right;\">             7</td><td style=\"text-align: right;\">          0.887462</td><td style=\"text-align: right;\">     0.00176246</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         9.03635</td><td style=\"text-align: right;\">   0.883333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_lgbm_5c1d5_00000:\n",
      "  date: 2022-08-08_12-29-14\n",
      "  done: false\n",
      "  experiment_id: d8826c2afcf148dc93bc8a79a0075820\n",
      "  hostname: MacBook.local\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 3160\n",
      "  time_since_restore: 10.048327922821045\n",
      "  time_this_iter_s: 0.11434602737426758\n",
      "  time_total_s: 10.048327922821045\n",
      "  timestamp: 1659986954\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 163\n",
      "  trial_id: 5c1d5_00000\n",
      "  valid_acc: 0.8833333333333333\n",
      "  warmup_time: 0.0021009445190429688\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-08 12:29:18 (running for 00:00:15.26)<br>Memory usage on this node: 12.0/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/17.23 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/kamil/ray_results/train_lgbm_2022-08-08_12-29-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  bagging_freq</th><th style=\"text-align: right;\">  feature_fraction</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  num_leaves</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  valid_acc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_lgbm_5c1d5_00000</td><td>RUNNING </td><td>127.0.0.1:3160</td><td style=\"text-align: right;\">             7</td><td style=\"text-align: right;\">          0.887462</td><td style=\"text-align: right;\">     0.00176246</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         14.0686</td><td style=\"text-align: right;\">   0.888889</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_lgbm_5c1d5_00000:\n",
      "  date: 2022-08-08_12-29-18\n",
      "  done: true\n",
      "  experiment_id: d8826c2afcf148dc93bc8a79a0075820\n",
      "  experiment_tag: 0_bagging_freq=7,feature_fraction=0.8875,learning_rate=0.0018,max_depth=7,num_leaves=20\n",
      "  hostname: MacBook.local\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 3160\n",
      "  time_since_restore: 14.761805772781372\n",
      "  time_this_iter_s: 0.13914799690246582\n",
      "  time_total_s: 14.761805772781372\n",
      "  timestamp: 1659986958\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 200\n",
      "  trial_id: 5c1d5_00000\n",
      "  valid_acc: 0.8888888888888888\n",
      "  warmup_time: 0.0021009445190429688\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-08 12:29:18 (running for 00:00:15.97)<br>Memory usage on this node: 12.0/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/17.23 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/kamil/ray_results/train_lgbm_2022-08-08_12-29-02<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  bagging_freq</th><th style=\"text-align: right;\">  feature_fraction</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  num_leaves</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  valid_acc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_lgbm_5c1d5_00000</td><td>TERMINATED</td><td>127.0.0.1:3160</td><td style=\"text-align: right;\">             7</td><td style=\"text-align: right;\">          0.887462</td><td style=\"text-align: right;\">     0.00176246</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">          20</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         14.7618</td><td style=\"text-align: right;\">   0.888889</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 12:29:18,949\tINFO tune.py:747 -- Total run time: 16.10 seconds (15.97 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(train_lgbm, config=search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When you call `tune.run()`, the trainable (`train_lgbm`) is evaluated with hyperparameters sampled from the search space (`search_space`).\n",
    "* Tune handles sampling and executing the trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display info about this trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>trial_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>...</th>\n",
       "      <th>config/bagging_freq</th>\n",
       "      <th>config/feature_fraction</th>\n",
       "      <th>config/learning_rate</th>\n",
       "      <th>config/max_depth</th>\n",
       "      <th>config/metric</th>\n",
       "      <th>config/num_class</th>\n",
       "      <th>config/num_leaves</th>\n",
       "      <th>config/objective</th>\n",
       "      <th>config/verbose</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.139148</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>5c1d5_00000</td>\n",
       "      <td>d8826c2afcf148dc93bc8a79a0075820</td>\n",
       "      <td>2022-08-08_12-29-18</td>\n",
       "      <td>1659986958</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.887462</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>7</td>\n",
       "      <td>multi_logloss</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-1</td>\n",
       "      <td>/Users/kamil/ray_results/train_lgbm_2022-08-08...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   valid_acc  time_this_iter_s   done  timesteps_total  episodes_total  \\\n",
       "0   0.888889          0.139148  False              NaN             NaN   \n",
       "\n",
       "   training_iteration     trial_id                     experiment_id  \\\n",
       "0                 200  5c1d5_00000  d8826c2afcf148dc93bc8a79a0075820   \n",
       "\n",
       "                  date   timestamp  ...  config/bagging_freq  \\\n",
       "0  2022-08-08_12-29-18  1659986958  ...                    7   \n",
       "\n",
       "   config/feature_fraction config/learning_rate config/max_depth  \\\n",
       "0                 0.887462             0.001762                7   \n",
       "\n",
       "   config/metric  config/num_class  config/num_leaves  config/objective  \\\n",
       "0  multi_logloss                10                 20        multiclass   \n",
       "\n",
       "   config/verbose                                             logdir  \n",
       "0              -1  /Users/kamil/ray_results/train_lgbm_2022-08-08...  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe(metric=\"valid_acc\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "We just ran tuning job with Tune 🚀.\n",
    "\n",
    "#### Key concepts in this section\n",
    "* Search space\n",
    "* Trainable\n",
    "* Trial\n",
    "\n",
    "#### Key API elements in this section\n",
    "* `ray.init()` -> start ray runtime.\n",
    "* `tune.report()` -> log the performance values. Called in the trainable function.\n",
    "* `tune.run()` -> execute tuning.\n",
    "\n",
    "#### Next\n",
    "We will modify `tune.run()` in order to run tuning with 100 trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Execute 100 tuning runs with Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-08 12:31:28 (running for 00:02:09.55)<br>Memory usage on this node: 11.7/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/17.23 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/kamil/ray_results/train_lgbm_2022-08-08_12-29-18<br>Number of trials: 100/100 (100 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 12:31:28,637\tINFO tune.py:747 -- Total run time: 129.66 seconds (129.54 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    train_lgbm,\n",
    "    config=search_space,\n",
    "    num_samples=100,\n",
    "    metric=\"valid_acc\",\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When `tune.run()` is called, trainable (`train_lgbm`) is evaluated `num_samples` times (100 trials) in parallel (subject to available compute resources).\n",
    "* Each trial has hyperparameters sampled from the search space (`search_space`).\n",
    "* Tune handles parallel execution, sampling from the search space and collecting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display info about best trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>trial_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>...</th>\n",
       "      <th>config/bagging_freq</th>\n",
       "      <th>config/feature_fraction</th>\n",
       "      <th>config/learning_rate</th>\n",
       "      <th>config/max_depth</th>\n",
       "      <th>config/metric</th>\n",
       "      <th>config/num_class</th>\n",
       "      <th>config/num_leaves</th>\n",
       "      <th>config/objective</th>\n",
       "      <th>config/verbose</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.094630</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>65b96_00002</td>\n",
       "      <td>c398ad3d7dfe445cbb36b9080edec8c9</td>\n",
       "      <td>2022-08-08_12-29-33</td>\n",
       "      <td>1659986973</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.618751</td>\n",
       "      <td>0.089884</td>\n",
       "      <td>7</td>\n",
       "      <td>multi_logloss</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-1</td>\n",
       "      <td>/Users/kamil/ray_results/train_lgbm_2022-08-08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.162196</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>65b96_00049</td>\n",
       "      <td>c597dea2c42e466f9fbff1db67b6cc7b</td>\n",
       "      <td>2022-08-08_12-30-36</td>\n",
       "      <td>1659987036</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.965862</td>\n",
       "      <td>0.050306</td>\n",
       "      <td>6</td>\n",
       "      <td>multi_logloss</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-1</td>\n",
       "      <td>/Users/kamil/ray_results/train_lgbm_2022-08-08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.961111</td>\n",
       "      <td>0.132065</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>65b96_00033</td>\n",
       "      <td>76b7af995a5f48abaec7b3460afb045f</td>\n",
       "      <td>2022-08-08_12-30-11</td>\n",
       "      <td>1659987011</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.623361</td>\n",
       "      <td>0.049349</td>\n",
       "      <td>5</td>\n",
       "      <td>multi_logloss</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-1</td>\n",
       "      <td>/Users/kamil/ray_results/train_lgbm_2022-08-08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.126503</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>65b96_00020</td>\n",
       "      <td>2b3318b14da9471f829ded3e19198ac8</td>\n",
       "      <td>2022-08-08_12-29-52</td>\n",
       "      <td>1659986992</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>0.789032</td>\n",
       "      <td>0.051331</td>\n",
       "      <td>4</td>\n",
       "      <td>multi_logloss</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-1</td>\n",
       "      <td>/Users/kamil/ray_results/train_lgbm_2022-08-08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.113440</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>65b96_00015</td>\n",
       "      <td>c398ad3d7dfe445cbb36b9080edec8c9</td>\n",
       "      <td>2022-08-08_12-29-45</td>\n",
       "      <td>1659986985</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>0.957914</td>\n",
       "      <td>0.049396</td>\n",
       "      <td>9</td>\n",
       "      <td>multi_logloss</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-1</td>\n",
       "      <td>/Users/kamil/ray_results/train_lgbm_2022-08-08...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    valid_acc  time_this_iter_s   done  timesteps_total  episodes_total  \\\n",
       "2    0.963889          0.094630  False              NaN             NaN   \n",
       "49   0.963889          0.162196  False              NaN             NaN   \n",
       "33   0.961111          0.132065  False              NaN             NaN   \n",
       "20   0.958333          0.126503  False              NaN             NaN   \n",
       "15   0.958333          0.113440  False              NaN             NaN   \n",
       "\n",
       "    training_iteration     trial_id                     experiment_id  \\\n",
       "2                  200  65b96_00002  c398ad3d7dfe445cbb36b9080edec8c9   \n",
       "49                 200  65b96_00049  c597dea2c42e466f9fbff1db67b6cc7b   \n",
       "33                 200  65b96_00033  76b7af995a5f48abaec7b3460afb045f   \n",
       "20                 200  65b96_00020  2b3318b14da9471f829ded3e19198ac8   \n",
       "15                 200  65b96_00015  c398ad3d7dfe445cbb36b9080edec8c9   \n",
       "\n",
       "                   date   timestamp  ...  config/bagging_freq  \\\n",
       "2   2022-08-08_12-29-33  1659986973  ...                    4   \n",
       "49  2022-08-08_12-30-36  1659987036  ...                   31   \n",
       "33  2022-08-08_12-30-11  1659987011  ...                   29   \n",
       "20  2022-08-08_12-29-52  1659986992  ...                   38   \n",
       "15  2022-08-08_12-29-45  1659986985  ...                   46   \n",
       "\n",
       "    config/feature_fraction config/learning_rate config/max_depth  \\\n",
       "2                  0.618751             0.089884                7   \n",
       "49                 0.965862             0.050306                6   \n",
       "33                 0.623361             0.049349                5   \n",
       "20                 0.789032             0.051331                4   \n",
       "15                 0.957914             0.049396                9   \n",
       "\n",
       "    config/metric  config/num_class  config/num_leaves  config/objective  \\\n",
       "2   multi_logloss                10                  9        multiclass   \n",
       "49  multi_logloss                10                 15        multiclass   \n",
       "33  multi_logloss                10                  8        multiclass   \n",
       "20  multi_logloss                10                 10        multiclass   \n",
       "15  multi_logloss                10                  6        multiclass   \n",
       "\n",
       "    config/verbose                                             logdir  \n",
       "2               -1  /Users/kamil/ray_results/train_lgbm_2022-08-08...  \n",
       "49              -1  /Users/kamil/ray_results/train_lgbm_2022-08-08...  \n",
       "33              -1  /Users/kamil/ray_results/train_lgbm_2022-08-08...  \n",
       "20              -1  /Users/kamil/ray_results/train_lgbm_2022-08-08...  \n",
       "15              -1  /Users/kamil/ray_results/train_lgbm_2022-08-08...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe(metric=\"valid_acc\")\n",
    "df.sort_values(by=[\"valid_acc\"], ascending=False).head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally you can use parallel coordinates plot to visualise results from all tuning runs. You can use [Plotly](https://plotly.com/python/parallel-coordinates-plot/) or [HiPlot](https://github.com/facebookresearch/hiplot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "We optimized hyperparameters by executing 100 tuning trials.\n",
    "\n",
    "#### Key API elements in this section\n",
    "* `tune.run(num_samples=...)` -> specify number of trials.\n",
    "\n",
    "#### Next\n",
    "We will introduce `scheduler` to early stop unpromising trials and as a result save compute time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ASHA with Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to ASHA (Asynchronous Successive Halving Algorithm)\n",
    "<img src=\"https://lh4.googleusercontent.com/E6KJ-5KQgfYVleJEXxaldICsEXm-dRUlsiD9AFbckXov0uaYfnIBKskLT6z1eLfptdKjxTCF05LBAz0W9evXbyWAViA5qYFGOaIYCuoz-h9n8rluHkl3ZOj-0IPKrdA4ES34Ybpo\" alt=\"synchronous promotions\" width=\"1000\">\n",
    "\n",
    "<img src=\"https://lh6.googleusercontent.com/ncYQXlFoVzhEsun2I-0LfTySEySc-uwEAd2vdPXGHvwprwXApuHuU4o17uJ1ITgHw9_sxId0995xOdfs-r7K3lWB4QQ7v9s33GnBs-EZ7cECIqj9Cq_eDQapJSAEG6P6A0oLZxm6\" alt=\"asynchronous promotions\" width=\"1000\">\n",
    "\n",
    "* Promote configurations whenever possible, hence utilize resources.\n",
    "* Asynchronous SHA utilizes resources efficiently. Workers are always busy by expanding the base rung if no configurations can be promoted to higher rungs.\n",
    "* Read more about ASHA in the CMU ML [blogpost](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/).\n",
    "\n",
    "_(Visualization is from the same [blogpost](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/). Date accessed: 2022.08.04)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ASHA from Tune schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ASHA scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "asha = ASHAScheduler(\n",
    "    time_attr=\"training_iteration\",\n",
    "    mode=\"max\",\n",
    "    grace_period=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run hyperparameter tuning with ASHA scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-08 12:31:51 (running for 00:00:22.26)<br>Memory usage on this node: 12.8/32.0 GiB<br>Using AsyncHyperBand: num_stopped=100\n",
       "Bracket: Iter 50.000: 0.8881944444444444<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/17.23 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/kamil/ray_results/train_lgbm_2022-08-08_12-31-28<br>Number of trials: 100/100 (100 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 12:31:51,279\tINFO tune.py:747 -- Total run time: 22.49 seconds (22.24 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    train_lgbm,\n",
    "    config=search_space,\n",
    "    num_samples=100,\n",
    "    metric=\"valid_acc\",\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    scheduler=asha,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display info about best trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>trial_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>...</th>\n",
       "      <th>config/bagging_freq</th>\n",
       "      <th>config/feature_fraction</th>\n",
       "      <th>config/learning_rate</th>\n",
       "      <th>config/max_depth</th>\n",
       "      <th>config/metric</th>\n",
       "      <th>config/num_class</th>\n",
       "      <th>config/num_leaves</th>\n",
       "      <th>config/objective</th>\n",
       "      <th>config/verbose</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.056654</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>b3189_00020</td>\n",
       "      <td>2cb8b1d634134602aa1daeaf09e0073f</td>\n",
       "      <td>2022-08-08_12-31-38</td>\n",
       "      <td>1659987098</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>0.724555</td>\n",
       "      <td>0.087994</td>\n",
       "      <td>4</td>\n",
       "      <td>multi_logloss</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-1</td>\n",
       "      <td>/Users/kamil/ray_results/train_lgbm_2022-08-08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.961111</td>\n",
       "      <td>0.059068</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>b3189_00044</td>\n",
       "      <td>2cb8b1d634134602aa1daeaf09e0073f</td>\n",
       "      <td>2022-08-08_12-31-43</td>\n",
       "      <td>1659987103</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.721172</td>\n",
       "      <td>0.090719</td>\n",
       "      <td>4</td>\n",
       "      <td>multi_logloss</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-1</td>\n",
       "      <td>/Users/kamil/ray_results/train_lgbm_2022-08-08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.961111</td>\n",
       "      <td>0.075981</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>b3189_00059</td>\n",
       "      <td>34e6c26a02284636901e639767d25b36</td>\n",
       "      <td>2022-08-08_12-31-47</td>\n",
       "      <td>1659987107</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.659131</td>\n",
       "      <td>0.086681</td>\n",
       "      <td>10</td>\n",
       "      <td>multi_logloss</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-1</td>\n",
       "      <td>/Users/kamil/ray_results/train_lgbm_2022-08-08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.069032</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>b3189_00003</td>\n",
       "      <td>34e6c26a02284636901e639767d25b36</td>\n",
       "      <td>2022-08-08_12-31-35</td>\n",
       "      <td>1659987095</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>0.096898</td>\n",
       "      <td>9</td>\n",
       "      <td>multi_logloss</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-1</td>\n",
       "      <td>/Users/kamil/ray_results/train_lgbm_2022-08-08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.035680</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>b3189_00017</td>\n",
       "      <td>059e756bb1f7418793145d52c9e3766c</td>\n",
       "      <td>2022-08-08_12-31-37</td>\n",
       "      <td>1659987097</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.935216</td>\n",
       "      <td>0.033308</td>\n",
       "      <td>3</td>\n",
       "      <td>multi_logloss</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>multiclass</td>\n",
       "      <td>-1</td>\n",
       "      <td>/Users/kamil/ray_results/train_lgbm_2022-08-08...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    valid_acc  time_this_iter_s  done  timesteps_total  episodes_total  \\\n",
       "20   0.963889          0.056654  True              NaN             NaN   \n",
       "44   0.961111          0.059068  True              NaN             NaN   \n",
       "59   0.961111          0.075981  True              NaN             NaN   \n",
       "3    0.955556          0.069032  True              NaN             NaN   \n",
       "17   0.950000          0.035680  True              NaN             NaN   \n",
       "\n",
       "    training_iteration     trial_id                     experiment_id  \\\n",
       "20                 100  b3189_00020  2cb8b1d634134602aa1daeaf09e0073f   \n",
       "44                 100  b3189_00044  2cb8b1d634134602aa1daeaf09e0073f   \n",
       "59                 100  b3189_00059  34e6c26a02284636901e639767d25b36   \n",
       "3                  100  b3189_00003  34e6c26a02284636901e639767d25b36   \n",
       "17                 100  b3189_00017  059e756bb1f7418793145d52c9e3766c   \n",
       "\n",
       "                   date   timestamp  ...  config/bagging_freq  \\\n",
       "20  2022-08-08_12-31-38  1659987098  ...                   40   \n",
       "44  2022-08-08_12-31-43  1659987103  ...                    4   \n",
       "59  2022-08-08_12-31-47  1659987107  ...                   17   \n",
       "3   2022-08-08_12-31-35  1659987095  ...                   36   \n",
       "17  2022-08-08_12-31-37  1659987097  ...                   35   \n",
       "\n",
       "    config/feature_fraction config/learning_rate config/max_depth  \\\n",
       "20                 0.724555             0.087994                4   \n",
       "44                 0.721172             0.090719                4   \n",
       "59                 0.659131             0.086681               10   \n",
       "3                  0.718070             0.096898                9   \n",
       "17                 0.935216             0.033308                3   \n",
       "\n",
       "    config/metric  config/num_class  config/num_leaves  config/objective  \\\n",
       "20  multi_logloss                10                  9        multiclass   \n",
       "44  multi_logloss                10                  9        multiclass   \n",
       "59  multi_logloss                10                 10        multiclass   \n",
       "3   multi_logloss                10                  8        multiclass   \n",
       "17  multi_logloss                10                  9        multiclass   \n",
       "\n",
       "    config/verbose                                             logdir  \n",
       "20              -1  /Users/kamil/ray_results/train_lgbm_2022-08-08...  \n",
       "44              -1  /Users/kamil/ray_results/train_lgbm_2022-08-08...  \n",
       "59              -1  /Users/kamil/ray_results/train_lgbm_2022-08-08...  \n",
       "3               -1  /Users/kamil/ray_results/train_lgbm_2022-08-08...  \n",
       "17              -1  /Users/kamil/ray_results/train_lgbm_2022-08-08...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe(metric=\"valid_acc\")\n",
    "df.sort_values(by=[\"valid_acc\"], ascending=False).head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "We ran hyperparameter tuning with 100 trials. ASHA scheduler terminated unpromising trials early. Saving compute resources.\n",
    "\n",
    "#### Key concepts in this section\n",
    "* Scheduler\n",
    "* Early stopping (of the unpromising trials)\n",
    "\n",
    "#### Key API elements in this section\n",
    "* `ASHAScheduler` -> [Async Successive Halving](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-ashascheduler) scheduler.\n",
    "* `tune.run(scheduler=...)` -> specify scheduler to use for tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disconnect the worker, and terminate processes started by `ray.init()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats!\n",
    "\n",
    "You just finished the micro tutorial on how to run and scale hyperparameter optimization with LightGBM and Tune.\n",
    "\n",
    "Now, please go to the [micro tutorial README](https://github.com/kamil-kaczmarek/ray-tune-micro-tutorial/blob/kk/dev/README.md), to learn more about next steps, and options to reach out and connect with the community."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
