{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "source": [
    "# Micro tutorial on how to run and scale hyperparameter optimization with LightGBM and Tune\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune_overview.png\" alt=\"Tune and integrations\" width=\"500\">\n",
    "\n",
    "Aug 2022. San Francisco, CA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: single LightGBM training session\n",
    "<img src=\"https://lightgbm.readthedocs.io/en/latest/_images/LightGBM_logo_black_text.svg\" alt=\"LightGBM Logo\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=7707)\n",
    "\n",
    "train_data = lgb.Dataset(data=X_train, label=y_train, free_raw_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use [digits dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) (classification) and create LightGBM Dataset object that will be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training parameters for single training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"num_class\": 10,\n",
    "    \"num_leaves\": 5,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"feature_fraction\": 0.5,\n",
    "    \"bagging_fraction\": 0.5,\n",
    "    \"bagging_freq\": 50,\n",
    "    \"max_depth\": 2,\n",
    "    \"verbose\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and train LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize booster\n",
    "gbm = lgb.Booster(params=training_parameters, train_set=train_data)\n",
    "\n",
    "# Train booster for 200 iterations\n",
    "for i in range(200):\n",
    "    gbm = lgb.train(\n",
    "        params=training_parameters,\n",
    "        train_set=train_data,\n",
    "        num_boost_round=1,\n",
    "        init_model=gbm,\n",
    "        keep_training_booster=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report accuracy on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(gbm.predict(X_valid), axis=1)\n",
    "acc = accuracy_score(y_true=y_valid, y_pred=y_pred)\n",
    "print(f\"Accuracy on valid set: {acc:.4f}, after {gbm.current_iteration()} iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* We just ran single LightGBM training session. To do that we prepared dataset and training hyperparameters.\n",
    "* Next, let's have a closer look at Tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2: Tune quickstart\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune.png\" alt=\"Tune logo\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Tune\n",
    "There are few components that we should look at first:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune_flow.png\" alt=\"Tune key concepts\" width=\"800\">\n",
    "\n",
    "Learn more about it from the [Key concepts](https://docs.ray.io/en/latest/tune/key-concepts.html) docs page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "cluster_info = ray.init()\n",
    "cluster_info.address_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster will be used for all tuning jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"num_class\": 10,\n",
    "    \"num_leaves\": tune.choice([2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 100]),\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"feature_fraction\": tune.uniform(0.5, 0.999),\n",
    "    \"bagging_fraction\": 0.5,\n",
    "    \"bagging_freq\": tune.randint(1, 50),\n",
    "    \"max_depth\": tune.randint(1, 11),\n",
    "    \"verbose\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(training_params, checkpoint_dir=None):\n",
    "    train_data = lgb.Dataset(data=X_train, label=y_train, free_raw_data=False)\n",
    "\n",
    "    # Initialize booster\n",
    "    gbm = lgb.Booster(params=training_params, train_set=train_data)\n",
    "\n",
    "    # Train booster for 200 iterations\n",
    "    for i in range(200):\n",
    "        gbm = lgb.train(\n",
    "            params=training_params,\n",
    "            train_set=train_data,\n",
    "            num_boost_round=1,\n",
    "            init_model=gbm,\n",
    "            keep_training_booster=True,\n",
    "        )\n",
    "\n",
    "        y_pred = np.argmax(gbm.predict(X_valid), axis=1)\n",
    "        acc = accuracy_score(y_true=y_valid, y_pred=y_pred)\n",
    "\n",
    "        # Send accuracy back to Tune\n",
    "        tune.report(valid_acc=acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run hyperparameter tuning, single trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(train_lgbm, config=search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display info about this trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe(metric=\"valid_acc\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* We just ran first trial using Tune.\n",
    "* Next, we will modify `tune.run()` in order to run tuning with 100 trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Execute 100 tuning runs with Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    train_lgbm,\n",
    "    config=search_space,\n",
    "    num_samples=100,\n",
    "    metric=\"valid_acc\",\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display info about best trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe(metric=\"valid_acc\")\n",
    "df.sort_values(by=[\"valid_acc\"], ascending=False).head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* We just optimized hyperparameters by executing 100 tuning trials.\n",
    "* Next, we will ontroduce `scheduler` to early stop not promising trials and as a result save compute time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ASHA with Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to ASHA (Asynchronous Successive Halving Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ASHA from Tune schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ASHA scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asha = ASHAScheduler(\n",
    "    time_attr=\"training_iteration\",\n",
    "    mode=\"max\",\n",
    "    grace_period=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run hyperparameter tuning with ASHA scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    train_lgbm,\n",
    "    config=search_space,\n",
    "    num_samples=100,\n",
    "    metric=\"valid_acc\",\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    scheduler=asha,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display info about best trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe(metric=\"valid_acc\")\n",
    "df.sort_values(by=[\"valid_acc\"], ascending=False).head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* We ran hyperparameter tuning with 100 trials. ASHA scheduler terminated not promising trials early. Saving compute resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown Ray cluster\n",
    "Shutdown ray cluster at the end of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats!\n",
    "\n",
    "You just finished the micro tutorial on how to run and scale hyperparameter optimization with LightGBM and Tune.\n",
    "\n",
    "Now, please go to the [micro tutorial README](https://github.com/kamil-kaczmarek/ray-tune-micro-tutorial/blob/kk/dev/README.md), to learn more about next steps, and options to reach out and connect with the community."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
